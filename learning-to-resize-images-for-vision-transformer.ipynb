{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Learning to Resize Images for Vision Transformer\n\n\nIn this notebook, it's shown how we can train a higher resolution image for vision transformer model. Usually transformer based models with high resolutoin may not fit into GPU. In such cases, we can adopt a **Trainable Resizer** mechanism as a backbone of the transformer models and perform as a joint  learning of the image resizer and recognition models.","metadata":{}},{"cell_type":"markdown","source":"[**Learning to Resize Images for Computer Vision Tasks** - Google Research](https://arxiv.org/pdf/2103.09950v1.pdf). For a given image resolutoin and a model, this research work answer how to best resize that image in a target resolutoin. Off-the-shelf image resizers for example: bilinear, bicubic methods are commonly used in most of the machine learning softwares. But this may limit the on-task performance of the trained model. In the above work, it's shown that typical linear resizer can be replaced with the **Learned Resizer**. Below is the overall proposed learnable resizer blocks.\n\n![image resizer](https://user-images.githubusercontent.com/17668390/138250657-29995830-b903-447f-8729-09b72b90ab3c.png)","metadata":{}},{"cell_type":"markdown","source":"In the paper, they showed that the proposed resizer mechanism improve the classificaiton mdoels. The added the resizer mechanism to the classification mdoels such as `DenseNet`, `InceptionNet` etc. IN this way, we can input very image size and the resizer mechanism will downsample the image appropriately for the actual mdoel. \n\n![rtee](https://user-images.githubusercontent.com/17668390/138254072-f87daa13-12cc-4c6a-9145-a567f644cb12.png)","metadata":{}},{"cell_type":"markdown","source":"[**Vision Transformer** - Google Research](https://arxiv.org/pdf/2010.11929.pdf). We know that the transformer models are computationally expensive. And thus limits the input size roughly around `224`, `384`. So, the idea is to use this **resizer mechanism** as a bacbone of the **vision transformer**, so that we can input enough large image for **joint learning**. So, the overall model architecture would be \n\n![Presentation2](https://user-images.githubusercontent.com/17668390/138256285-c24f98db-ce35-4877-8741-221fd57d895e.jpg)","metadata":{}},{"cell_type":"markdown","source":"**Reference**\n\n- [ROBIN SMITS](https://www.kaggle.com/rsmits/effnet-b2-feature-models-catboost#SET-TPU-/-GPU) - For general training pipelines. Great work. \n- [Learnable-Image-Resizing](https://github.com/sayakpaul/Learnable-Image-Resizing) For resizer building blocks. \n- [TensorFlow-HUB](https://github.com/sayakpaul/ViT-jax2tf) For ViT ","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport tensorflow as tf \nfrom tensorflow.keras import Input, Model, Sequential, layers\nimport tensorflow_hub as hub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-22T11:39:37.800221Z","iopub.execute_input":"2021-10-22T11:39:37.800737Z","iopub.status.idle":"2021-10-22T11:39:43.081612Z","shell.execute_reply.started":"2021-10-22T11:39:37.800616Z","shell.execute_reply":"2021-10-22T11:39:43.080838Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"INP_SIZE      = (512, 512) # Input size of the Image Resizer Module (IRM)\nTARGET_SIZE   = (224, 224) # Output size of IRM and Input size of the Vision Transformer \nINTERPOLATION = \"bilinear\"","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:39:43.083320Z","iopub.execute_input":"2021-10-22T11:39:43.083580Z","iopub.status.idle":"2021-10-22T11:39:43.088605Z","shell.execute_reply.started":"2021-10-22T11:39:43.083544Z","shell.execute_reply":"2021-10-22T11:39:43.087824Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**tf.data**","metadata":{}},{"cell_type":"code","source":"Q = 30\nfeature_folds = 10\n\nbatch_size = 12\nepochs  = 10\nseed  = 123123\nverbose = 1\nlr  = 0.005\n\nDATA_DIR = '../input/petfinder-pawpularity-score/'\nTRAIN_DIR = DATA_DIR + 'train/'\nTEST_DIR = DATA_DIR + 'test/'\n\n# SetAutoTune\nAUTOTUNE = tf.data.experimental.AUTOTUNE  \n\ndef build_augmenter(is_labelled):\n    def augment(img):\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_saturation(img, 0.95, 1.05)\n        img = tf.image.random_brightness(img, 0.05)\n        img = tf.image.random_contrast(img, 0.95, 1.05)\n        img = tf.image.random_hue(img, 0.05)\n        return img\n    def augment_with_labels(img, label):\n        return augment(img), label\n    return augment_with_labels if is_labelled else augment\n\ndef build_decoder(is_labelled):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        img = tf.image.decode_jpeg(file_bytes, channels = 3)\n        img = tf.image.resize(img, (INP_SIZE))\n        return img\n    def decode_with_labels(path, label):\n        return decode(path), label\n    return decode_with_labels if is_labelled else decode\n\ndef create_dataset(df, batch_size = 32, is_labelled = False, \n                   augment = False, repeat = False, shuffle = False):\n    decode_fn    = build_decoder(is_labelled)\n    augmenter_fn = build_augmenter(is_labelled)\n    \n    # Create Dataset\n    if is_labelled:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values, df['target_value'].values))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values))\n    dataset = dataset.map(decode_fn, num_parallel_calls = AUTOTUNE)\n    dataset = dataset.map(augmenter_fn, num_parallel_calls = AUTOTUNE) if augment else dataset\n    dataset = dataset.repeat() if repeat else dataset\n    dataset = dataset.shuffle(1024, reshuffle_each_iteration = True) if shuffle else dataset\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-22T11:39:43.090033Z","iopub.execute_input":"2021-10-22T11:39:43.090585Z","iopub.status.idle":"2021-10-22T11:39:43.107477Z","shell.execute_reply.started":"2021-10-22T11:39:43.090494Z","shell.execute_reply":"2021-10-22T11:39:43.106812Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load Train Data\ntrain_df = pd.read_csv(f'{DATA_DIR}train.csv')\ntrain_df['Id'] = train_df['Id'].apply(lambda x: f'{TRAIN_DIR}{x}.jpg')\n\n# Set a specific label to be able to perform stratification\ntrain_df['stratify_label'] = pd.qcut(train_df['Pawpularity'], q = Q, labels = range(Q))\n\n# Label value to be used for feature model 'classification' training.\ntrain_df['target_value'] = train_df['Pawpularity'] / 100.\n\n# Summary\nprint(f'train_df: {train_df.shape}')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:39:43.108830Z","iopub.execute_input":"2021-10-22T11:39:43.109100Z","iopub.status.idle":"2021-10-22T11:39:43.193629Z","shell.execute_reply.started":"2021-10-22T11:39:43.109058Z","shell.execute_reply":"2021-10-22T11:39:43.192894Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Learning to Resize\n\n![image resizer](https://user-images.githubusercontent.com/17668390/138250657-29995830-b903-447f-8729-09b72b90ab3c.png)","metadata":{}},{"cell_type":"code","source":"# ref: https://keras.io/examples/vision/learnable_resizer/\ndef residual_block(x):\n    shortcut = x\n\n    def conv_bn_leaky(inputs, filters, kernel_size, strides):\n        x = layers.Conv2D(filters, kernel_size, strides=strides, \n                          use_bias=False, padding='same')(inputs)\n        x = layers.BatchNormalization()(x)\n        x = layers.LeakyReLU()(x)\n        return x \n    \n    def conv_bn(inputs, filters, kernel_size, strides):\n        x = layers.Conv2D(filters, kernel_size, strides, padding='same')(inputs)\n        x = layers.BatchNormalization()(x)\n        return x \n    \n    x = conv_bn_leaky(x, 16, 3, 1)\n    x = conv_bn(x, 16, 3, 1)\n    x = layers.add([shortcut, x])\n    return x\n\n\ndef get_learnable_resizer(filters=16, num_res_blocks=1, interpolation=INTERPOLATION):\n    inputs = layers.Input(shape=[None, None, 3])\n\n    # First, perform naive resizing.\n    naive_resize = layers.Resizing(\n        *TARGET_SIZE, interpolation=interpolation\n    )(inputs)\n\n    # First convolution block without batch normalization.\n    x = layers.Conv2D(filters=filters, kernel_size=7, strides=1, padding='same')(inputs)\n    x = layers.LeakyReLU(0.2)(x)\n\n    # Second convolution block with batch normalization.\n    x = layers.Conv2D(filters=filters, kernel_size=1, strides=1, padding='same')(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.BatchNormalization()(x)\n\n    # Intermediate resizing as a bottleneck.\n    bottleneck = layers.Resizing(\n        *TARGET_SIZE, interpolation=interpolation\n    )(x)\n    \n    # Residual passes.\n    x = residual_block(bottleneck)\n    for i in range(1, num_res_blocks):\n        x = residual_block(x)\n        \n    # Projection.\n    x = layers.Conv2D(\n        filters=filters, kernel_size=3, strides=1, padding=\"same\", use_bias=False\n    )(x)\n    x = layers.BatchNormalization()(x)\n\n    # Skip connection.\n    x = layers.Add()([bottleneck, x])\n\n    # Final resized image.\n    x = layers.Conv2D(filters=3, kernel_size=7, strides=1, padding=\"same\")(x)\n    final_resize = layers.Add()([naive_resize, x])\n    return Model(inputs, final_resize, name=\"learnable_resizer\")\n\nlearnable_resizer = get_learnable_resizer(num_res_blocks=3)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:39:43.197225Z","iopub.execute_input":"2021-10-22T11:39:43.197568Z","iopub.status.idle":"2021-10-22T11:39:45.874376Z","shell.execute_reply.started":"2021-10-22T11:39:43.197523Z","shell.execute_reply":"2021-10-22T11:39:45.873589Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Check**\n\nLet's check how raw image get tansformed with this resizer blocks with initial states.","metadata":{}},{"cell_type":"code","source":"training_dataset = create_dataset(train_df,\n                                  batch_size  = batch_size, \n                                  is_labelled = True, \n                                  augment = True,\n                                  repeat  = True, \n                                  shuffle = True)\nsample_images, _ = next(iter(training_dataset))","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:39:45.876211Z","iopub.execute_input":"2021-10-22T11:39:45.877002Z","iopub.status.idle":"2021-10-22T11:40:00.176172Z","shell.execute_reply.started":"2021-10-22T11:39:45.876963Z","shell.execute_reply":"2021-10-22T11:40:00.175328Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt \n\nplt.figure(figsize=(16, 10))\nfor i, image in enumerate(sample_images[:6]):\n    image = image / 255\n\n    ax = plt.subplot(3, 4, 2 * i + 1)\n    plt.title(\"Input Image\")\n    plt.imshow(image.numpy().squeeze())\n    plt.axis(\"off\")\n\n    ax = plt.subplot(3, 4, 2 * i + 2)\n    resized_image = learnable_resizer(image[None, ...])\n    plt.title(\"Resized Image\")\n    plt.imshow(resized_image.numpy().squeeze())\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:40:00.178541Z","iopub.execute_input":"2021-10-22T11:40:00.179135Z","iopub.status.idle":"2021-10-22T11:40:07.679095Z","shell.execute_reply.started":"2021-10-22T11:40:00.179091Z","shell.execute_reply":"2021-10-22T11:40:07.678393Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Learned Resizer + Vision Transformer ","metadata":{}},{"cell_type":"code","source":"handle=\"https://tfhub.dev/sayakpaul/vit_s16_fe/1\"\n\ndef get_model(plot_modal, print_summary, with_compile):\n    hub_layer = hub.KerasLayer(handle, trainable=True)\n    backbone = Sequential(\n        [\n            layers.InputLayer((TARGET_SIZE[0], TARGET_SIZE[1], 3)),\n            hub_layer\n        ], name='vit'\n    )\n    inputs = layers.Input((INP_SIZE[0], INP_SIZE[1], 3))\n    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n    x = learnable_resizer(x)\n    outputs = backbone(x)\n    \n    tail = Sequential(\n            [\n                layers.Dropout(0.2),\n                layers.BatchNormalization(),\n                layers.Dense(1, activation = 'sigmoid')\n            ], name='head'\n        )\n    \n    model = Model(inputs, tail(outputs))\n    \n    if plot_modal:\n        display(tf.keras.utils.plot_model(model, show_shapes=True, \n                                          show_layer_names=True,  expand_nested=True))\n    if print_summary:\n        print(model.summary())\n        \n    if with_compile:\n        model.compile(\n            optimizer = tf.keras.optimizers.Adam(learning_rate = lr),  \n            loss = tf.keras.losses.BinaryCrossentropy(), \n            metrics = [tf.keras.metrics.RootMeanSquaredError('rmse')])  \n    return model ","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:40:07.680673Z","iopub.execute_input":"2021-10-22T11:40:07.681189Z","iopub.status.idle":"2021-10-22T11:40:07.706314Z","shell.execute_reply.started":"2021-10-22T11:40:07.681147Z","shell.execute_reply":"2021-10-22T11:40:07.705620Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"get_model(plot_modal=True, print_summary=True, with_compile=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:40:07.707985Z","iopub.execute_input":"2021-10-22T11:40:07.708644Z","iopub.status.idle":"2021-10-22T11:40:19.888339Z","shell.execute_reply.started":"2021-10-22T11:40:07.708600Z","shell.execute_reply":"2021-10-22T11:40:19.887646Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import losses, optimizers, metrics\nfrom tensorflow.keras import callbacks\n\ndef model_callback(fold):\n    ckpt = tf.keras.callbacks.ModelCheckpoint(f'feature_model_{fold}.h5',\n                                              verbose = 1, \n                                              monitor = 'val_rmse',\n                                              mode = 'min', \n                                              save_weights_only = True,\n                                              save_best_only = True)\n    \n    return [ckpt]","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:40:19.890530Z","iopub.execute_input":"2021-10-22T11:40:19.891022Z","iopub.status.idle":"2021-10-22T11:40:19.897575Z","shell.execute_reply.started":"2021-10-22T11:40:19.890981Z","shell.execute_reply":"2021-10-22T11:40:19.896933Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import gc\nfrom sklearn.model_selection import StratifiedKFold\n\n# OOF RMSE Placeholder\nall_val_rmse = []\nkfold = StratifiedKFold(n_splits = feature_folds, \n                        shuffle = True, random_state = seed)\nfor fold, (train_index, val_index) in enumerate(kfold.split(train_df.index,\n                                                            train_df['stratify_label'])):\n    if fold == 0:\n        print(f'\\nFold {fold}\\n')\n        # Pre model.fit cleanup\n        tf.keras.backend.clear_session()\n        gc.collect()\n\n        # Create Model\n        model = get_model(plot_modal    = False, \n                          print_summary = False,\n                          with_compile  = True)\n        for i in range(len(model.weights)):\n            model.weights[i]._handle_name = model.weights[i].name + str(i)\n    \n        # Create TF Datasets\n        trn = train_df.iloc[train_index]\n        val = train_df.iloc[val_index]\n        training_dataset = create_dataset(trn, \n                                          batch_size  = batch_size, \n                                          is_labelled = True, \n                                          augment     = True, \n                                          repeat      = True, \n                                          shuffle     = True)\n        validation_dataset = create_dataset(val, \n                                            batch_size  = batch_size, \n                                            is_labelled = True,\n                                            augment     = False, \n                                            repeat      = True,\n                                            shuffle     = False)\n        # Fit Model\n        history = model.fit(training_dataset,\n                            epochs = epochs,\n                            steps_per_epoch  = trn.shape[0] // batch_size,\n                            validation_steps = val.shape[0] // batch_size,\n                            callbacks = model_callback(fold),\n                            validation_data = validation_dataset,\n                            verbose = 2)   \n\n        # Validation Information\n        best_val_rmse = min(history.history['val_rmse'])\n        all_val_rmse.append(best_val_rmse)\n        print(f'\\nValidation RMSE: {best_val_rmse}\\n')\n        del model ","metadata":{"execution":{"iopub.status.busy":"2021-10-22T11:40:19.899618Z","iopub.execute_input":"2021-10-22T11:40:19.900198Z"},"trusted":true},"execution_count":null,"outputs":[]}]}